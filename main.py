#!/usr/bin/env python3
"""
THREATfox Indicator Checker
Exports THREATfox indicators to CSV files based on configured queries.
"""

import os
import sys
import csv
import json
import requests
import yaml
import datetime
import hashlib
import argparse
import re
from pathlib import Path
from dotenv import load_dotenv


class ThreatFoxChecker:
    """
    ThreatFoxChecker fetches indicators from THREATfox API
    and exports them to CSV files.
    """

    BASE_URL = "https://threatfox-api.abuse.ch/api/v1/"
    
    def __init__(self, test_mode=False):
        """
        Initialize the ThreatFoxChecker.
        Loads configuration from .env and config.yaml.
        
        Args:
            test_mode (bool): If True, cache API responses for testing
        """
        self.test_mode = test_mode
        
        # Create cache directory if in test mode
        if self.test_mode:
            self.cache_dir = Path("cache")
            self.cache_dir.mkdir(exist_ok=True)
        
        # Create output directory for CSV files
        self.output_dir = Path("output")
        self.output_dir.mkdir(exist_ok=True)
        
        # Load environment variables from .env file
        load_dotenv()
        
        # Get API key from environment variables
        self.api_key = os.getenv("THREATFOX_API_KEY")
        if not self.api_key and not self.test_mode:
            print("Error: THREATFOX_API_KEY not found in .env file")
            sys.exit(1)
            
        # Load configuration from config.yaml
        try:
            with open("config.yaml", "r") as f:
                self.config = yaml.safe_load(f)
        except FileNotFoundError:
            print("Error: config.yaml not found. Please rename config.yaml.example to config.yaml.")
            sys.exit(1)
        except yaml.YAMLError as e:
            print(f"Error parsing config.yaml: {e}")
            sys.exit(1)
            
        # Validate configuration
        if "queries" not in self.config or not self.config["queries"]:
            print("Error: No queries found in config.yaml")
            sys.exit(1)
            
    def run(self):
        """
        Run the ThreatFox checker for all configured queries.
        """
        for query in self.config["queries"]:
            try:
                # Validate query
                self._validate_query(query)
                
                # Get indicators for query
                indicators = self._get_indicators(query)
                
                # Export indicators to CSV
                if indicators:
                    self._export_to_csv(query, indicators)
                else:
                    print(f"No indicators found for query: {query['name']}")
            except Exception as e:
                print(f"Error processing query {query.get('name', 'unknown')}: {e}")
    
    def _validate_query(self, query):
        """
        Validate a query configuration.
        """
        required_fields = ["name", "type", "value", "limiter"]
        for field in required_fields:
            if field not in query:
                raise ValueError(f"Required field '{field}' missing in query")
                
        # Validate query type
        valid_types = ["tag", "malware_name"]
        if query["type"] not in valid_types:
            raise ValueError(f"Invalid query type: {query['type']}. Must be one of {valid_types}")
            
        # Validate limiter format
        limiter = query["limiter"]
        if not isinstance(limiter, (int, str)):
            raise ValueError("Limiter must be an integer representing max results to fetch")
        
        # Ensure limiter is numeric
        try:
            int(limiter)
        except ValueError:
            raise ValueError("Limiter must be a numeric value")
    
    def _generate_cache_key(self, payload):
        """
        Generate a unique cache key based on the request payload.
        
        Args:
            payload (dict): The API request payload
            
        Returns:
            str: A cache key based on the hash of the payload
        """
        # Convert payload to sorted JSON string to ensure consistent hashing
        payload_str = json.dumps(payload, sort_keys=True)
        # Create hash
        return hashlib.md5(payload_str.encode()).hexdigest()
    
    def _get_cache_path(self, cache_key):
        """
        Get the cache file path for a given cache key.
        
        Args:
            cache_key (str): The cache key generated by _generate_cache_key
            
        Returns:
            Path: Path to the cache file
        """
        return self.cache_dir / f"{cache_key}.json"
    
    def _get_indicators(self, query):
        """
        Get indicators from THREATfox API based on the query.
        Uses cached responses in test mode if available.
        """
        print(f"Fetching indicators for query: {query['name']}")
        
        # Prepare request payload based on query type
        if query["type"] == "tag":
            payload = {
                "query": "taginfo",
                "tag": query["value"]
            }
        elif query["type"] == "malware_name":
            payload = {
                "query": "malwareinfo",
                "malware": query["value"]
            }
        
        # Set limiter
        limiter = query["limiter"]
        try:
            # Use limit parameter to limit number of results
            payload["limit"] = min(int(limiter), 1000)  # API max is 1000
        except (ValueError, TypeError):
            # Default to 100 if limiter is not a valid integer
            payload["limit"] = 100
            
        # Check cache first if in test mode
        if self.test_mode:
            cache_key = self._generate_cache_key(payload)
            cache_path = self._get_cache_path(cache_key)
            
            if cache_path.exists():
                print(f"Using cached response for query: {query['name']}")
                try:
                    with open(cache_path, 'r') as f:
                        data = json.load(f)
                        if data["query_status"] == "ok" and "data" in data:
                            return data["data"]
                except (json.JSONDecodeError, KeyError) as e:
                    print(f"Error reading cache file: {e}")
                    # Continue with API request if cache is invalid
            
        # Make request to API
        headers = {
            "Accept": "application/json",
            "Auth-Key": self.api_key
        }
        
        try:
            print(f"Making API request to ThreatFox API for {query['name']}")
            response = requests.post(
                self.BASE_URL,
                headers=headers,
                data=json.dumps(payload)
            )
            
            # Check response
            response.raise_for_status()  # Raises an exception for 4XX/5XX responses
            
            # Parse response
            data = response.json()
            
            # Save response to cache if in test mode
            if self.test_mode:
                cache_key = self._generate_cache_key(payload)
                cache_path = self._get_cache_path(cache_key)
                with open(cache_path, 'w') as f:
                    json.dump(data, f, indent=2)
                print(f"Cached API response to {cache_path}")
            
            if data["query_status"] != "ok":
                error_message = data.get("data", {}).get("error_message", "Unknown error")
                print(f"API Error: {error_message}")
                return None
                
            # Return indicators
            if "data" not in data or not data["data"]:
                return None
                
            return data["data"]
            
        except requests.exceptions.RequestException as e:
            print(f"Request error: {e}")
            return None
        except json.JSONDecodeError:
            print("Error: Invalid JSON response from API")
            return None
    def _export_to_csv(self, query, indicators):
        """
        Export indicators to multiple CSV files based on their types.
        Creates separate files for each indicator type and saves them in the output directory.
        Updates existing files with new indicators and removes duplicates.
        """
        # Group indicators by type
        grouped_indicators = self._group_indicators_by_type(indicators)
        
        print(f"Exporting {len(indicators)} indicators for query: {query['name']}")
        
        # Create a combined file with all indicators
        self._write_combined_csv(query['name'], indicators)
        
        # Export each indicator type to a separate file
        for indicator_type, type_indicators in grouped_indicators.items():
            if type_indicators:
                self._write_type_specific_csv(query['name'], indicator_type, type_indicators)
    def _group_indicators_by_type(self, indicators):
        """
        Group indicators by their type (domain, ip, url, ip:port, etc.)
        and create special groupings as needed.
        """
        grouped = {}
        
        # IP addresses without ports (extracted from ip:port indicators)
        ips_only = []
        
        for indicator in indicators:
            ioc_type = indicator.get("ioc_type", "unknown")
            
            # Initialize the group if it doesn't exist
            if ioc_type not in grouped:
                grouped[ioc_type] = []
                
            # Add indicator to its type group
            grouped[ioc_type].append(indicator)
            
            # Special handling for ip:port to extract just IPs
            if ioc_type == "ip:port":
                # Create a copy of the indicator for the IP-only list
                ip_only_indicator = indicator.copy()
                # Extract just the IP part
                ip = indicator.get("ioc", "").split(":")[0] if ":" in indicator.get("ioc", "") else indicator.get("ioc", "")
                ip_only_indicator["ioc"] = ip
                ip_only_indicator["ioc_type"] = "ip"
                ips_only.append(ip_only_indicator)
        
        # Add the IP-only group if we have any
        if ips_only:
            grouped["ip"] = ips_only
            
        return grouped
    def _load_existing_csv(self, filename, fields):
        """
        Load existing indicators from a CSV file if it exists.
        Returns a dictionary of indicators keyed by id.
        """
        existing_indicators = {}
        
        if filename.exists():
            try:
                with open(filename, "r", newline="", encoding="utf-8") as f:
                    reader = csv.DictReader(f)
                    # Verify the CSV structure
                    if reader.fieldnames == fields:
                        for row in reader:
                            # Use the indicator ID as the key
                            if "id" in row and row["id"]:
                                existing_indicators[row["id"]] = row
            except Exception as e:
                print(f"Warning: Could not read existing file {filename}: {e}")
                
        return existing_indicators
    
    def _write_combined_csv(self, query_name, indicators):
        """
        Write all indicators to a combined CSV file.
        Merges with existing indicators and removes duplicates.
        """
        # Define filename
        filename = self.output_dir / f"{query_name}.csv"
        
        # Define CSV fields
        fields = [
            "id", "ioc_type", "ioc_value", "threat_type", "threat_type_desc",
            "malware", "tags", "reporter", "confidence_level",
            "first_seen", "last_seen"
        ]
        
        # Load existing indicators, if any
        existing_indicators = self._load_existing_csv(filename, fields)
        
        # Prepare the merged dataset
        merged_indicators = existing_indicators.copy()
        new_count = 0
        
        # Add new indicators, overwriting existing ones with same ID
        for indicator in indicators:
            row = self._create_row_data(indicator)
            indicator_id = str(row["id"])
            if indicator_id not in merged_indicators:
                new_count += 1
            merged_indicators[indicator_id] = row
        
        # Write merged indicators back to CSV
        with open(filename, "w", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=fields)
            writer.writeheader()
            
            # Sort by ID to maintain consistent order
            for indicator_id in sorted(merged_indicators.keys()):
                writer.writerow(merged_indicators[indicator_id])
        
        # Report what was updated
        if existing_indicators:
            print(f"Updated {filename} with {new_count} new indicators, {len(merged_indicators)} total")
        else:
            print(f"Created {filename} with {len(merged_indicators)} indicators")
        
    def _write_type_specific_csv(self, query_name, indicator_type, indicators):
        """
        Write indicators of a specific type to a CSV file.
        Merges with existing indicators and removes duplicates.
        """
        # Format the indicator type for the filename (convert special characters)
        safe_type = indicator_type.replace(":", "-")
        
        # Define filename
        filename = self.output_dir / f"{query_name}-{safe_type}.csv"
        
        # Define CSV fields
        fields = [
            "id", "ioc_value", "threat_type", "threat_type_desc",
            "malware", "tags", "reporter", "confidence_level",
            "first_seen", "last_seen"
        ]
        
        # Load existing indicators, if any
        existing_indicators = self._load_existing_csv(filename, fields)
        
        # Prepare the merged dataset
        merged_indicators = existing_indicators.copy()
        new_count = 0
        
        # Add new indicators, overwriting existing ones with same ID
        for indicator in indicators:
            row = self._create_row_data(indicator)
            # Remove the ioc_type field since it's redundant in type-specific files
            if "ioc_type" in row:
                del row["ioc_type"]
                
            indicator_id = str(row["id"])
            if indicator_id not in merged_indicators:
                new_count += 1
            merged_indicators[indicator_id] = row
        
        # Write merged indicators back to CSV
        with open(filename, "w", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=fields)
            writer.writeheader()
            
            # Sort by ID to maintain consistent order
            for indicator_id in sorted(merged_indicators.keys()):
                writer.writerow(merged_indicators[indicator_id])
        
        # Report what was updated
        if existing_indicators:
            print(f"Updated {filename} with {new_count} new {indicator_type} indicators, {len(merged_indicators)} total")
        else:
            print(f"Created {filename} with {len(merged_indicators)} {indicator_type} indicators")
        
    def _create_row_data(self, indicator):
        """
        Create a row dictionary for a CSV file from an indicator.
        """
        row = {
            "id": indicator.get("id", ""),
            "ioc_type": indicator.get("ioc_type", ""),
            "ioc_value": indicator.get("ioc", ""),
            "threat_type": indicator.get("threat_type", ""),
            "threat_type_desc": indicator.get("threat_type_desc", ""),
            "malware": indicator.get("malware_printable", ""),
            "confidence_level": indicator.get("confidence_level", ""),
            "first_seen": indicator.get("first_seen", ""),
            "last_seen": indicator.get("last_seen", ""),
            "reporter": indicator.get("reporter", "")
        }
        
        # Handle tags
        if "tags" in indicator and indicator["tags"]:
            row["tags"] = ", ".join(indicator["tags"])
        else:
            row["tags"] = ""
            
        return row


if __name__ == "__main__":
    # Parse command line arguments
    parser = argparse.ArgumentParser(description='ThreatFox Checker - Export ThreatFox indicators to CSV')
    parser.add_argument('--test', action='store_true', help='Enable test mode (cache API responses)')
    args = parser.parse_args()
    
    # Initialize checker with test mode if specified
    checker = ThreatFoxChecker(test_mode=args.test)
    checker.run()
